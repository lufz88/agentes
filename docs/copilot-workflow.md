# ğŸ¤– Workflow con GitHub Copilot + Claude para Agentes de IA

## FilosofÃ­a: Copilot como tu "Pair Programmer" para Agentes

Copilot con Claude entiende arquitecturas agÃ©nticas. No lo uses solo para autocompletar â€” Ãºsalo como un **debugger de razonamiento** y **arquitecto de sistemas**.

---

## 1. Debugear Reasoning Loops (Chain of Thought)

### Prompt: "Analiza mi reasoning loop"

Cuando tu agente entra en loop infinito o toma decisiones incorrectas, pega el historial de mensajes y pregunta:

```
AquÃ­ estÃ¡ el historial de mi agente (messages array).
El agente deberÃ­a haber respondido al usuario pero en su lugar
siguiÃ³ llamando al tool "get_weather" 5 veces seguidas.

[pega el JSON del historial]

Â¿Por quÃ© el LLM sigue en loop? Â¿QuÃ© cambio en el system prompt
o tool descriptions lo arreglarÃ­a?
```

**Lo que Claude hace bien aquÃ­:**
- Identifica si la tool description es ambigua
- Detecta si el tool result no da suficiente info al LLM para "salir" del loop
- Sugiere cambios al system prompt para establecer criterios de parada

### Prompt: "Traza la ejecuciÃ³n"

```
Traza paso a paso quÃ© harÃ­a mi agente con este input:
"Compara el clima de Madrid y Barcelona y dime cuÃ¡l es mejor para ir de vacaciones"

Dado este system prompt: [...]
Y estos tools: [...]

Muestra cada iteraciÃ³n del reasoning loop:
1. QuÃ© piensa el LLM
2. QuÃ© tool_calls genera
3. QuÃ© resultados recibe
4. Si decide seguir o responder
```

---

## 2. DiseÃ±ar Tool Definitions

### Prompt: "Genera el schema del tool"

```
Necesito un tool para mi agente que consulte la base de datos de PostgreSQL.
El agente deberÃ­a poder:
- Hacer queries SELECT
- Filtrar por fecha, categorÃ­a y rango de precio
- Limitar resultados

Genera:
1. La ToolDefinition con JSON Schema completo
2. El Zod schema para validaciÃ³n
3. La funciÃ³n execute con manejo de errores
4. Tests del tool
```

### Prompt: "Optimiza mis descriptions"

```
Estos son mis tool definitions actuales.
El LLM a veces usa "calculator" cuando deberÃ­a usar "search_documents"
y viceversa.

[pega las definitions]

Reescribe las descriptions para que el LLM pueda distinguir
claramente cuÃ¡ndo usar cada uno. Incluye ejemplos de uso.
```

---

## 3. DiseÃ±ar System Prompts

### Prompt: "System prompt para agente especializado"

```
DiseÃ±a un system prompt para un agente que:
- Tiene acceso a: [lista de tools]
- Su dominio es: [ej: anÃ¡lisis financiero]
- Debe seguir estas reglas: [ej: siempre citar fuentes]
- Tono: [ej: profesional pero accesible]

Incluye:
1. DescripciÃ³n del rol
2. Lista de capacidades
3. Reglas de comportamiento
4. Manejo de errores
5. Formato de respuesta
```

---

## 4. Arquitectura Multi-Agente

### Prompt: "DiseÃ±a la orquestaciÃ³n"

```
Necesito un sistema multi-agente para [caso de uso].

Tengo estos agentes especializados:
- Investigador: busca en documentos
- Analista: procesa datos numÃ©ricos
- Redactor: genera reportes

DiseÃ±a:
1. El Router Agent (prompt + lÃ³gica de routing)
2. El protocolo de comunicaciÃ³n entre agentes
3. El formato de "handoff" (cÃ³mo un agente pasa contexto a otro)
4. Manejo de fallos y fallbacks
```

---

## 5. Debugging en Vivo con Copilot Chat

### Workflow recomendado:

1. **Selecciona cÃ³digo** del reasoning loop â†’ `Ctrl+Shift+I` (inline chat)
2. Pregunta: "Â¿Es posible que este loop no termine? Â¿Bajo quÃ© condiciones?"
3. Copilot analiza el flujo y seÃ±ala edge cases

### Para errores de tool calling:

```
Este tool retorna el siguiente error al agente:
{"error": "Cannot read property 'temperature' of undefined"}

El LLM luego intenta llamar al mismo tool con los mismos args.
Â¿CÃ³mo hago que el agente se recupere de errores de tools?
MuÃ©strame el patrÃ³n de retry con backoff y fallback.
```

---

## 6. Testing de Agentes

### Prompt: "Test del reasoning loop"

```
Escribe tests para mi agente que verifiquen:
1. Que usa el tool correcto para cada tipo de pregunta
2. Que no entra en loops infinitos (max iterations)
3. Que maneja errores de tools gracefully
4. Que el historial se mantiene correctamente
5. Que el agente puede encadenar mÃºltiples tools

Usa vitest/jest. Mock el LLM client para controlar las respuestas.
```

---

## 7. Prompts Avanzados para Copilot

### Generar un tool completo desde un API spec

```
AquÃ­ estÃ¡ el OpenAPI spec de mi backend:
[pega el YAML/JSON]

Genera tools para mi agente que cubran estos endpoints.
Cada tool debe:
- Tener una ToolDefinition con description detallada
- Validar args con Zod
- Manejar errores HTTP
- Retornar datos formateados para el LLM
```

### Optimizar token usage

```
Mi agente consume muchos tokens porque el historial crece rÃ¡pido.
Los tool results son muy largos (JSON de APIs).

MuÃ©strame cÃ³mo implementar:
1. Summarization del historial (comprimir mensajes antiguos)
2. Truncation inteligente de tool results
3. Sliding window de contexto
4. EstimaciÃ³n de tokens antes de la llamada al LLM
```

### Migrar de OpenAI Functions a Tool Calling genÃ©rico

```
Tengo este cÃ³digo que usa openai.chat.completions con functions (legacy).
Migra a la API de tools actual, manteniendo compatibilidad con:
- OpenAI
- Anthropic (Claude)
- Ollama (local)

[pega el cÃ³digo]
```

---

## Cheatsheet de Prompts para el Chat

| SituaciÃ³n | Prompt |
|---|---|
| Loop infinito | "Â¿Por quÃ© mi agente sigue ejecutando tools sin responder? [historial]" |
| Tool incorrecto | "El LLM elige X cuando deberÃ­a elegir Y. Â¿CÃ³mo mejoro las descriptions?" |
| Respuesta pobre | "El agente usa la tool correcta pero da mala respuesta. Â¿El system prompt estÃ¡ mal?" |
| Error de parsing | "El LLM genera JSON malformado en los tool args. Â¿CÃ³mo hago retry?" |
| Performance | "Mi reasoning loop es lento. Â¿Puedo ejecutar tools en paralelo?" |
| Arquitectura | "Â¿DeberÃ­a usar un solo agente con 15 tools o 3 agentes especializados?" |
| State | "Â¿CÃ³mo persisto el state del agente entre requests HTTP?" |
| Streaming | "Â¿CÃ³mo hago streaming del reasoning loop al frontend?" |

---

## Tip Final: Usa el Chat de Copilot como "LLM Inspector"

Cuando desarrollas agentes, *tÃº eres el humano observando cÃ³mo otro LLM razona*. 
Copilot/Claude es perfecto para esta meta-tarea:

> "Pon teniÃ©ndote como un LLM que recibe este prompt y estos tools.
> Â¿QuÃ© tool_calls generarÃ­as? Â¿Por quÃ©? Â¿Hay ambigÃ¼edad?"

Esto te da **visibilidad** sobre el razonamiento del LLM sin gastar tokens en OpenAI.
