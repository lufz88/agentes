# ğŸ“– Summary â€” Conceptos Clave de Arquitecturas AgÃ©nticas

> Archivo de referencia para el workspace **Zero to Hero: Arquitecturas AgÃ©nticas**.
> Los comentarios del cÃ³digo fuente referencian secciones de este archivo usando el formato:
> `(ver docs/summary.md#nombre-de-seccion)`

---

## Tabla de contenidos

1. [Â¿QuÃ© es un Agente?](#que-es-un-agente)
2. [Reasoning Loop](#reasoning-loop)
3. [Tool Calling](#tool-calling)
4. [Zod y validaciÃ³n runtime](#zod-y-validacion-runtime)
5. [Prompt Engineering para agentes](#prompt-engineering-para-agentes)
6. [RAG (Retrieval-Augmented Generation)](#rag-retrieval-augmented-generation)
   - [Embeddings](#embeddings)
   - [Chunking](#chunking)
   - [Vector Store y similitud coseno](#vector-store-y-similitud-coseno)
   - [El pipeline completo](#el-pipeline-completo-rag)
7. [SSE (Server-Sent Events)](#sse-server-sent-events)
8. [Generative UI](#generative-ui)
9. [State Management agÃ©ntico (Zustand)](#state-management-agÃ©ntico-zustand)
10. [Multi-Agent Orchestration](#multi-agent-orchestration)
11. [Tool Calling: TypeScript vs Java (Spring AI)](#tool-calling-typescript-vs-java)
12. [Patrones de error y recuperaciÃ³n](#patrones-de-error-y-recuperacion)
13. [Multi-Provider: configuraciÃ³n compartida](#multi-provider-configuracion-compartida)
14. [CÃ³mo testear el proyecto 03 (Java RAG)](#como-testear-proyecto-03)
15. [Glosario rÃ¡pido](#glosario-rapido)

---

## Â¿QuÃ© es un Agente? {#que-es-un-agente}

Un **agente** es un programa que usa un LLM (Large Language Model) como "cerebro" para tomar
decisiones en un loop, con la capacidad de ejecutar acciones en el mundo real a travÃ©s de **tools**.

La diferencia clave entre un agente y un simple chatbot:

| | Chatbot | Agente |
|---|---|---|
| **Flujo** | Input â†’ LLM â†’ Output | Input â†’ LLM â†’ (Tool? â†’ Observe â†’ LLM)* â†’ Output |
| **Decisiones** | Ninguna | Elige quÃ© tools usar, cuÃ¡ndo, y en quÃ© orden |
| **Iteraciones** | 1 (single-shot) | N (loop hasta tener suficiente info) |
| **Estado** | Sin estado (o estado mÃ­nimo) | Historial completo + resultados de tools |
| **Capacidades** | Solo texto | Texto + acciones (APIs, archivos, cÃ¡lculos, UI, etc.) |

### AnalogÃ­a

PensÃ¡ en un agente como un analista que tiene acceso a herramientas:
- Le hacÃ©s una pregunta
- El analista **piensa** quÃ© informaciÃ³n necesita
- **Usa** sus herramientas (buscar en documentos, hacer cÃ¡lculos)
- **Observa** los resultados
- Decide si necesita mÃ¡s info o ya puede responder
- Te da una **respuesta fundamentada**

El LLM es el "cerebro" del analista. Los tools son sus herramientas. El reasoning loop
es su proceso de pensamiento.

### En este proyecto

- **`01-cli-agent`**: Agente bÃ¡sico con tools de clima, calculadora y filesystem
- **`02-generative-ui`**: Agente que ademÃ¡s controla componentes de UI
- **`03-java-rag-agent`**: Agente con acceso a documentos (RAG) y orquestaciÃ³n multi-agente

---

## Reasoning Loop {#reasoning-loop}

El **reasoning loop** es el corazÃ³n de cualquier agente. Es un ciclo que se repite hasta que el
agente tiene suficiente informaciÃ³n para responder:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                            â”‚
â”‚   USER INPUT                                               â”‚
â”‚       â”‚                                                    â”‚
â”‚       â–¼                                                    â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”    Â¿tool_calls?   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚   â”‚ THINK  â”‚â”€â”€â”€â”€ SÃ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚   ACT    â”‚               â”‚
â”‚   â”‚        â”‚                    â”‚(ejecutar â”‚               â”‚
â”‚   â”‚(llamar â”‚                    â”‚ tools)   â”‚               â”‚
â”‚   â”‚ al LLM)â”‚                    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜               â”‚
â”‚   â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                         â”‚                     â”‚
â”‚       â”‚                         â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”               â”‚
â”‚       â”‚ NO                      â”‚ OBSERVE  â”‚               â”‚
â”‚    (content)                    â”‚(aÃ±adir   â”‚               â”‚
â”‚       â”‚                         â”‚resultadosâ”‚               â”‚
â”‚       â–¼                         â”‚al histor.)â”‚              â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”                    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜               â”‚
â”‚   â”‚  DONE  â”‚                         â”‚                     â”‚
â”‚   â”‚(respon-â”‚             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â”‚   â”‚ der al â”‚             â”‚ (volver al LLM                  â”‚
â”‚   â”‚usuario)â”‚             â”‚  con los resultados)            â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ THINK             â”‚
â”‚                                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### CÃ³mo funciona paso a paso

1. **THINK**: Se envÃ­a el historial completo (mensajes del usuario + resultados previos de tools)
   al LLM junto con las definiciones de tools disponibles.

2. **El LLM responde de una de dos formas**:
   - `content`: texto para el usuario â†’ **DONE**
   - `tool_calls`: array de funciones que quiere ejecutar â†’ **ACT**

3. **ACT**: Se ejecuta cada tool call. Los argumentos vienen como JSON string del LLM.

4. **OBSERVE**: Los resultados de los tools se agregan al historial como mensajes con
   `role: "tool"`. Esto es clave: el LLM **ve** estos resultados en la siguiente iteraciÃ³n.

5. **Loop**: Se vuelve al paso 1. Ahora el LLM tiene mÃ¡s contexto y puede decidir:
   - Usar otro tool (necesita mÃ¡s info)
   - Responder al usuario (ya tiene suficiente)

### ProtecciÃ³n anti-loop infinito

Es crÃ­tico tener un **mÃ¡ximo de iteraciones**. Si el LLM se confunde, podrÃ­a llamar tools
indefinidamente. En este proyecto:

- `01-cli-agent`: mÃ¡ximo **10** iteraciones (`agent.ts L62`)
- `02-generative-ui`: mÃ¡ximo **8** iteraciones (`agent.ts L101`)

### Â¿Por quÃ© el LLM "entiende" los resultados?

Porque los resultados de tools se agregan al historial de mensajes como `role: "tool"`,
con un `tool_call_id` que vincula la respuesta a la llamada original. El LLM fue entrenado
para interpretar este formato y usar la informaciÃ³n en sus siguientes decisiones.

### Ejemplo de historial en una iteraciÃ³n tÃ­pica

```json
[
  { "role": "system", "content": "Eres un asistente..." },
  { "role": "user", "content": "Â¿QuÃ© clima hace en Madrid?" },
  { "role": "assistant", "tool_calls": [{ "id": "call_1", "function": { "name": "get_weather", "arguments": "{\"city\":\"Madrid\"}" }}] },
  { "role": "tool", "tool_call_id": "call_1", "content": "{\"temperature\":22,\"condition\":\"soleado\"}" },
  { "role": "assistant", "content": "En Madrid hace 22Â°C y estÃ¡ soleado." }
]
```

NotÃ¡ que el historial crece con cada iteraciÃ³n. Esto es lo que da al agente "memoria"
dentro de un turno.

---

## Tool Calling {#tool-calling}

**Tool calling** es el mecanismo por el cual un LLM puede "ejecutar funciones" en tu sistema.
El LLM **no ejecuta cÃ³digo**: genera un JSON declarando quÃ© funciÃ³n quiere llamar y con quÃ©
argumentos. Tu cÃ³digo es el responsable de ejecutar la funciÃ³n y devolver el resultado.

### Las 3 partes de un Tool

Cada tool tiene 3 componentes:

#### 1. DefiniciÃ³n (JSON Schema) â€” Lo que el LLM ve

```typescript
const definition: ToolDefinition = {
  name: "get_weather",           // Nombre Ãºnico
  description: "Obtiene el clima actual de una ciudad...", // CRÃTICO: el LLM usa esto para decidir
  parameters: {
    type: "object",
    properties: {
      city: { type: "string", description: "Nombre de la ciudad" },
      units: { type: "string", enum: ["celsius", "fahrenheit"] }
    },
    required: ["city"]
  }
};
```

La `description` es el factor mÃ¡s importante. Una mala descripciÃ³n = el LLM no sabe cuÃ¡ndo
usar el tool. Reglas para buenas descripciones:
- DecÃ­ **cuÃ¡ndo** usar el tool, no solo quÃ© hace
- IncluÃ­ ejemplos de inputs
- SÃ© especÃ­fico sobre limitaciones

#### 2. ValidaciÃ³n con Zod â€” ProtecciÃ³n runtime

Los argumentos del LLM llegan como JSON string. Pueden estar mal formados.
Zod los valida antes de ejecutar (ver [#zod-y-validacion-runtime]).

#### 3. FunciÃ³n execute â€” La lÃ³gica real

La funciÃ³n que realmente hace el trabajo. Recibe los argumentos validados y retorna un string
(los LLMs trabajan con texto).

### tool_choice: "auto" vs "required" vs "none"

- `"auto"` (default): El LLM decide si usar tools o responder directamente
- `"required"`: El LLM DEBE usar al menos un tool (Ãºtil para forzar acciones)
- `"none"`: El LLM NO puede usar tools (Ãºtil para forzar texto)

### Tool calls paralelos

El LLM puede generar **mÃºltiples tool_calls** en una sola respuesta. Ejemplo: si le pedÃ­s
"Â¿clima en Madrid y Buenos Aires?", puede generar dos `get_weather` simultÃ¡neos.
En `01-cli-agent`, estos se ejecutan secuencialmente en un `for`. En producciÃ³n, se puede usar
`Promise.all` para paralelizar.

### Capacidades emergentes: encadenamiento de tools

Algo fascinante: el LLM puede descubrir por sÃ­ mismo que puede **encadenar** tools.
Si le pedÃ­s "Â¿quÃ© archivos hay en /tmp y leÃ© el mÃ¡s reciente?", el agente:
1. Usa `list_directory` para ver los archivos
2. Ve el resultado
3. Usa `read_file` con el archivo mÃ¡s reciente

Nadie le programÃ³ esta secuencia. La deduce del contexto. Esto se llama
**capacidad emergente** y es uno de los aspectos mÃ¡s potentes de los agentes.

---

## Zod y validaciÃ³n runtime {#zod-y-validacion-runtime}

### Â¿QuÃ© es Zod?

[Zod](https://zod.dev) es una librerÃ­a de TypeScript para **validaciÃ³n y parsing de datos en
runtime**. A diferencia de los tipos de TypeScript (que existen solo en compilaciÃ³n y luego
desaparecen), Zod valida datos reales en tiempo de ejecuciÃ³n.

### Â¿Por quÃ© se necesita en un agente?

El LLM genera argumentos como **JSON strings**. Estos pueden tener errores:

```
LLM genera:  { "city": "Madrid", "units": "kelvin" }  â† "kelvin" no es vÃ¡lido
Esperabas:   { "city": "Madrid", "units": "celsius" | "fahrenheit" }
```

TypeScript no puede detectar esto en runtime porque sus tipos se borran despuÃ©s de compilar.
Zod sÃ­ puede:

```typescript
const ArgsSchema = z.object({
  city: z.string().min(1),
  units: z.enum(["celsius", "fahrenheit"]).default("celsius"),
});

ArgsSchema.parse(args); // â† Tira ZodError si "kelvin" viene como valor
```

### Dual-layer de validaciÃ³n

En este proyecto se usan **dos capas** de validaciÃ³n:

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  JSON Schema en ToolDefinition   â”‚  â† El LLM LEE esto
                    â”‚  (lo que el LLM ve)              â”‚     para generar args
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                    LLM genera JSON string
                               â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Zod Schema (.parse())            â”‚  â† Tu cÃ³digo EJECUTA esto
                    â”‚  (validaciÃ³n runtime)             â”‚     antes de usar los args
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                    Args validados â†’ execute()
```

- **Capa 1 (JSON Schema)**: Le dice al LLM quÃ© estructura crear. Es "advisory" â€” el LLM
  puede ignorarlo o equivocarse.
- **Capa 2 (Zod)**: Valida lo que el LLM realmente generÃ³. Es "enforcing" â€” si falla,
  el error se devuelve al LLM para que corrija.

### MÃ©todos clave de Zod en este proyecto

| MÃ©todo | QuÃ© hace | Ejemplo |
|--------|----------|---------|
| `z.string()` | Valida que sea string | `z.string().min(1)` |
| `z.number()` | Valida que sea nÃºmero | `z.number().min(-90).max(90)` |
| `z.enum()` | Valida que sea uno de los valores | `z.enum(["celsius","fahrenheit"])` |
| `z.object()` | Valida un objeto con schema | `z.object({ city: z.string() })` |
| `.default()` | Valor por defecto si falta | `.default("celsius")` |
| `.describe()` | DocumentaciÃ³n (no afecta validaciÃ³n) | `.describe("ExpresiÃ³n matemÃ¡tica")` |
| `.parse()` | Valida y retorna tipado â€” tira error si falla | `schema.parse(input)` |
| `.safeParse()` | Igual pero retorna `{ success, data, error }` | `schema.safeParse(input)` |

### `.describe()` vs `description` en JSON Schema

Son cosas distintas:
- **`z.string().describe("...")`**: Metadata para desarrolladores o generaciÃ³n automÃ¡tica de schemas.
  No llega al LLM directamente.
- **`description` en ToolDefinition**: Texto que el LLM lee para entender el parÃ¡metro.

En este proyecto, los JSON Schemas se escriben manualmente (no se auto-generan desde Zod),
asÃ­ que `.describe()` es solo documentaciÃ³n interna.

---

## Prompt Engineering para agentes {#prompt-engineering-para-agentes}

DiseÃ±ar prompts para agentes es diferente a diseÃ±ar prompts para chatbots.
Un agente necesita instrucciones claras sobre **cuÃ¡ndo y cÃ³mo usar tools**.

### AnatomÃ­a de un system prompt agÃ©ntico

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. ROL                                       â”‚
â”‚    "Eres un asistente inteligente con acceso  â”‚
â”‚     a herramientas."                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2. CAPACIDADES (quÃ© tools tiene)             â”‚
â”‚    "- Clima: consultar el clima              â”‚
â”‚     - Calculadora: evaluar expresiones       â”‚
â”‚     - File System: leer/escribir archivos"   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 3. REGLAS (cuÃ¡ndo y cÃ³mo actuar)             â”‚
â”‚    "1. SIEMPRE usa herramientas para datos   â”‚
â”‚     2. Puedes usar MÃšLTIPLES tools           â”‚
â”‚     3. Explica tu razonamiento               â”‚
â”‚     4. Si falla, intenta otro enfoque"       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 4. FORMATO/ESTILO                            â”‚
â”‚    "Responde en espaÃ±ol.                     â”‚
â”‚     Piensa paso a paso."                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Diferencias clave entre los 3 proyectos

| Proyecto | Foco del prompt | InstrucciÃ³n especial |
|----------|-----------------|----------------------|
| `01-cli-agent` | Tools genÃ©ricos | "Piensa paso a paso para problemas complejos" |
| `02-generative-ui` | Componentes visuales | "Es mejor mostrar que contar" â€” prioriza UI |
| `03-java-rag-agent` | Documentos y fuentes | "Fundamenta tus respuestas en el contexto proporcionado" |
| Router Agent (03) | ClasificaciÃ³n | "Responde SOLO con: RAG, DATA, o SUMMARY" |

### Tips concretos

1. **Listar las tools explÃ­citamente** en el system prompt. No alcanza con que estÃ©n en
   `tools`: el LLM funciona mejor si su prompt textual las describe.

2. **"Piensa paso a paso"** (chain-of-thought) mejora el razonamiento del agente
   significativamente, especialmente en tareas complejas.

3. **Instrucciones de fallback**: "Si algo falla, intenta un enfoque alternativo"
   le da al agente permiso para recuperarse de errores.

4. **Restricciones de output** son Ãºtiles para el Router Agent: forzar que responda
   SOLO con una palabra reduce alucinaciones en tareas de clasificaciÃ³n.

---

## RAG (Retrieval-Augmented Generation) {#rag-retrieval-augmented-generation}

**RAG** es un patrÃ³n que le da al LLM acceso a **informaciÃ³n que no tiene** en su entrenamiento.
En lugar de fine-tunear el modelo (costoso, lento), le "inyectamos" contexto relevante en el prompt.

### Â¿Por quÃ© RAG?

Los LLMs tienen limitaciones:
- **Corte de conocimiento**: Solo saben lo que vieron en el entrenamiento (ej: GPT-4 no
  sabe quÃ© pasÃ³ ayer)
- **Sin datos privados**: No conocen los documentos internos de tu empresa
- **Alucinaciones**: Cuando no saben algo, inventan con confianza

RAG resuelve las 3: le das documentos actuales y privados, y el LLM responde basÃ¡ndose
en ellos (no inventa).

### El pipeline completo RAG {#el-pipeline-completo-rag}

```
    FASE OFFLINE (una vez, batch)           FASE ONLINE (por cada consulta)
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•             â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Documentos â”‚                          â”‚ Query del    â”‚
    â”‚ (PDF, TXT, â”‚                          â”‚ usuario      â”‚
    â”‚  DOCX...)  â”‚                          â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                                 â”‚
           â”‚                                       â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”                          â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  1. PARSE  â”‚                          â”‚  2. EMBED    â”‚
    â”‚  (Tika)    â”‚                          â”‚  query       â”‚
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                          â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                                       â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”                          â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  2. CHUNK  â”‚                          â”‚  3. SEARCH   â”‚
    â”‚(TokenText  â”‚                          â”‚  (similitud  â”‚
    â”‚ Splitter)  â”‚                          â”‚   coseno)    â”‚
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                          â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                                       â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”                          â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  3. EMBED  â”‚                          â”‚  4. AUGMENT  â”‚
    â”‚  (vectores)â”‚                          â”‚  (contexto + â”‚
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                          â”‚   query)     â”‚
           â”‚                                â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”                                 â”‚
    â”‚  4. STORE  â”‚                          â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  (Vector   â”‚                          â”‚  5. GENERATE â”‚
    â”‚   Store)   â”‚                          â”‚  (LLM con    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â”‚   contexto)  â”‚
                                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Embeddings {#embeddings}

Un **embedding** es una representaciÃ³n numÃ©rica (vector) del significado de un texto.

```
"El gato estÃ¡ en el techo"  â†’  [0.23, -0.87, 0.12, 0.45, ..., -0.33]  (1536 dimensiones)
"El felino estÃ¡ arriba"     â†’  [0.21, -0.85, 0.14, 0.43, ..., -0.31]  (vectores similares!)
"Receta de pastel"          â†’  [0.95, 0.12, -0.74, 0.08, ..., 0.67]  (vector muy diferente)
```

Los textos con significado similar producen vectores cercanos en el espacio.
Esto permite **buscar por significado** en lugar de por palabras exactas.

**Modelo usado en `03-java-rag-agent`**: `text-embedding-3-small` de OpenAI â€” genera
vectores de 1536 dimensiones. Es rÃ¡pido y barato, ideal para desarrollo.

**Â¿CÃ³mo se generan?**: Spring AI los genera automÃ¡ticamente al agregar documentos al
Vector Store y al hacer bÃºsquedas. No necesitÃ¡s llamar a la API de embeddings manualmente.

### Chunking {#chunking}

Los documentos largos no se pueden meter completos en un prompt (lÃ­mite de tokens) ni
generar un solo embedding (pierde detalle). La soluciÃ³n: dividirlos en **chunks**.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Documento original (50 pÃ¡ginas)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                    chunk_size=800
                    chunk_overlap=200
                          â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Chunk 1  â”‚ â”‚ Chunk 2  â”‚ â”‚ Chunk 3  â”‚ â”‚ Chunk N  â”‚
    â”‚ 800 tok  â”‚ â”‚ 800 tok  â”‚ â”‚ 800 tok  â”‚ â”‚ â‰¤800 tok â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â—„â”€â”€200â”€â”€â–º     â—„â”€â”€200â”€â”€â–º     â—„â”€â”€200â”€â”€â–º
            overlap       overlap       overlap
```

**Â¿Por quÃ© overlap?** Sin overlap, una oraciÃ³n que cruza el lÃ­mite entre dos chunks
queda cortada y pierde sentido. Con overlap de 200 tokens, los Ãºltimos 200 del chunk N
son los primeros 200 del chunk N+1. AsÃ­ la informaciÃ³n de borde se preserva.

**ConfiguraciÃ³n en `03-java-rag-agent`** (`application.yml`):
- `chunk-size: 800` â€” tamaÃ±o de cada chunk en tokens
- `chunk-overlap: 200` â€” cuÃ¡ntos tokens se solapan entre chunks consecutivos

**Trade-offs del chunk size**:
- **Muy chico** (100-200): bÃºsqueda precisa pero pierde contexto
- **Muy grande** (2000+): mucho contexto pero mezcla temas, y ocupa mÃ¡s prompt
- **Sweet spot** (500-1000): balance entre precisiÃ³n y contexto

### Vector Store y similitud coseno {#vector-store-y-similitud-coseno}

Un **Vector Store** es una base de datos optimizada para buscar por similitud entre vectores.

**CÃ³mo funciona la bÃºsqueda**:
1. La consulta del usuario se convierte a un embedding (vector)
2. Se compara ese vector contra todos los vectores almacenados
3. Se retornan los mÃ¡s similares (medido por **similitud coseno**)

**Similitud coseno** mide el Ã¡ngulo entre dos vectores:
- **1.0** = idÃ©nticos (mismo significado)
- **0.0** = sin relaciÃ³n
- El `threshold: 0.7` en el proyecto filtra resultados con poca relevancia

**En `03-java-rag-agent`**: Se usa `SimpleVectorStore` (en memoria), ideal para desarrollo.
En producciÃ³n se usan bases de datos especializadas: **PgVector** (PostgreSQL), **Pinecone**,
**Qdrant**, **Chroma**, **Weaviate**, etc.

---

## SSE (Server-Sent Events) {#sse-server-sent-events}

**Server-Sent Events** es un protocolo HTTP unidireccional (servidor â†’ cliente) para
enviar eventos en tiempo real. A diferencia de WebSockets, SSE:
- Usa HTTP estÃ¡ndar (no necesita protocolo especial)
- Es unidireccional (solo servidor â†’ cliente)
- Se reconecta automÃ¡ticamente si se corta
- Funciona con proxies y firewalls sin problemas

### Â¿Por quÃ© SSE en un agente?

Un reasoning loop puede tardar segundos por iteraciÃ³n. Sin streaming, el usuario
ve una pantalla vacÃ­a hasta que el agente termina. Con SSE, el frontend muestra
progreso en **tiempo real**:

```
Backend                           Frontend
â”€â”€â”€â”€â”€â”€â”€â”€                          â”€â”€â”€â”€â”€â”€â”€â”€
IteraciÃ³n 1                       
  â”‚ emit("thinking")   â”€â”€â”€â”€â”€â”€â–¶   ğŸ”„ Pensando...
  â”‚ emit("tool_call")  â”€â”€â”€â”€â”€â”€â–¶   ğŸ”§ Usando: show_weather_card
  â”‚ emit("ui_action")  â”€â”€â”€â”€â”€â”€â–¶   ğŸ“Š [Monta WeatherCard]
  â”‚ emit("tool_result") â”€â”€â”€â”€â”€â–¶   âœ… Resultado recibido
IteraciÃ³n 2
  â”‚ emit("thinking")   â”€â”€â”€â”€â”€â”€â–¶   ğŸ”„ Pensando...
  â”‚ emit("text")       â”€â”€â”€â”€â”€â”€â–¶   ğŸ’¬ "El clima en Madrid es..."
  â”‚ emit("done")       â”€â”€â”€â”€â”€â”€â–¶   âœ… Fin del turno
```

### Formato de un evento SSE

```
data: {"type":"thinking","data":{"iteration":1},"timestamp":1708099200000}\n\n
```

Cada evento es una lÃ­nea `data: <JSON>\n\n`. El doble `\n\n` marca el fin del evento.

### Â¿Por quÃ© `fetch` + `ReadableStream` y no `EventSource`?

El frontend de `02-generative-ui` **no** usa la API nativa [`EventSource`](https://developer.mozilla.org/en-US/docs/Web/API/EventSource)
porque `EventSource` solo soporta requests **GET**. El endpoint `/api/chat` es **POST**
(envÃ­a el mensaje en el body). Por eso se usa `fetch` + `ReadableStream` + `TextDecoder`
para parsear el stream manualmente.

### Headers SSE requeridos

```typescript
res.writeHead(200, {
  "Content-Type": "text/event-stream",  // Indica formato SSE
  "Cache-Control": "no-cache",          // No cachear eventos
  "Connection": "keep-alive",           // Mantener conexiÃ³n abierta
});
```

---

## Generative UI {#generative-ui}

**Generative UI** es un patrÃ³n donde **el agente decide quÃ© componentes de UI renderizar**,
no el usuario ni el desarrollador. El agente genera tanto texto como instrucciones para
montar/actualizar/desmontar componentes React.

### Diferencia con UI tradicional

```
UI TRADICIONAL:
  Usuario clickea botÃ³n â†’ handler â†’ setState â†’ render

GENERATIVE UI:
  Usuario escribe texto â†’ Agente â†’ tool_call â†’ UIAction â†’ render
  (el agente decide QUÃ‰ componente mostrar basÃ¡ndose en el contexto)
```

### El patrÃ³n dual-return

Cada tool de UI retorna **dos cosas**:

```typescript
return {
  toolResult: JSON.stringify(data),   // Para el LLM (texto plano)
  uiAction: {                          // Para el frontend (componente React)
    type: "mount",
    componentId: "weather-madrid",
    component: "weather_card",
    props: { city: "Madrid", temperature: 22, ... }
  }
};
```

- **`toolResult`**: String que se envÃ­a al LLM en el historial. El LLM lo usa para
  razonar sobre los datos y generar su texto de respuesta.
- **`uiAction`**: Objeto que se envÃ­a al frontend via SSE. Describe quÃ© componente
  montar y con quÃ© props.

### UIAction: mount / update / unmount

| Tipo | QuÃ© hace | CuÃ¡ndo se usa |
|------|----------|---------------|
| `mount` | Crea y muestra un componente nuevo | Primera vez que aparece |
| `update` | Actualiza props de un componente existente | Datos nuevos para un componente existente |
| `unmount` | Quita un componente de la UI | Ya no es relevante |

### Component Registry (Factory Pattern)

El frontend tiene un mapeo de tipo string â†’ componente React:

```typescript
const COMPONENT_REGISTRY = {
  weather_card: WeatherCard,
  chart: Chart,
  data_table: DataTable,
};
```

Cuando llega una UIAction con `component: "weather_card"`, el `DynamicComponent` busca
en el registry y renderiza `<WeatherCard {...props} />`. Este patrÃ³n es extensible:
para agregar un componente nuevo, solo hay que agregarlo al registry y crear un tool
que lo produzca.

---

## State Management agÃ©ntico (Zustand) {#state-management-agÃ©ntico-zustand}

### La inversiÃ³n del flujo de datos

En una app React tradicional:

```
Usuario interactÃºa â†’ dispatch/setState â†’ UI se re-renderiza
```

En Generative UI, **el agente controla el state**:

```
Agente decide â†’ SSE event â†’ store.mountComponent() â†’ UI se re-renderiza
```

El usuario solo escribe texto. Todo lo demÃ¡s (quÃ© componentes se muestran, quÃ© datos
tienen, cuÃ¡ndo se actualizan) lo decide el agente.

### Â¿Por quÃ© Zustand?

[Zustand](https://zustand-demo.pmnd.rs/) es un state manager para React, minimalista
y sin boilerplate. Comparado con Redux:

| | Redux | Zustand |
|---|---|---|
| Boilerplate | Actions, reducers, dispatch | Function calls directas |
| Setup | createStore, Provider | `create()` â€” una funciÃ³n |
| Selectors | useSelector + memoization | Hook nativo con selector |
| Devtools | Extension aparte | Integrado |

Para un agente, Zustand es ideal porque:
- Las acciones del store se invocan directamente desde el hook SSE
- No necesitÃ¡s middleware para side effects (el SSE ya es el "side effect")
- Los componentes se suscriben solo a lo que necesitan (reactivo y eficiente)

### Shape del store

```typescript
interface AgentState {
  messages: ChatMessage[];           // Historial del chat
  mountedComponents: MountedComponent[]; // Componentes activos
  isThinking: boolean;               // Â¿Agente procesando?
  currentTool: string | null;        // Tool en ejecuciÃ³n
  error: string | null;              // Ãšltimo error
}
```

### Flujo completo: SSE â†’ Store â†’ UI

```
Backend                  Hook (useAgentSSE)         Store (Zustand)          React
â”€â”€â”€â”€â”€â”€â”€                  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€         â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€          â”€â”€â”€â”€â”€
emit("thinking")    â”€â”€â–¶  handleEvent()         â”€â”€â–¶  setThinking(true)   â”€â”€â–¶  ğŸ”„ spinner
emit("tool_call")   â”€â”€â–¶  handleEvent()         â”€â”€â–¶  setCurrentTool(name)â”€â”€â–¶  ğŸ”§ badge
emit("ui_action")   â”€â”€â–¶  handleEvent()         â”€â”€â–¶  mountComponent(...)  â”€â”€â–¶  ğŸ“Š component!
emit("text")        â”€â”€â–¶  handleEvent()         â”€â”€â–¶  addMessage(content)  â”€â”€â–¶  ğŸ’¬ text
emit("done")        â”€â”€â–¶  handleEvent()         â”€â”€â–¶  setThinking(false)   â”€â”€â–¶  âœ… idle
```

La asociaciÃ³n componente-mensaje es importante: cuando se monta un componente, tambiÃ©n
se asocia al Ãºltimo mensaje del assistant (via `msg.components`). AsÃ­, al scrollear el
chat, cada mensaje muestra sus componentes correspondientes.

---

## Multi-Agent Orchestration {#multi-agent-orchestration}

### Â¿Por quÃ© mÃºltiples agentes?

Un solo agente con muchas tools presenta problemas:
- **ConfusiÃ³n del LLM**: Con 20+ tools, el LLM elige mal cuÃ¡l usar
- **Prompts genÃ©ricos**: No se puede optimizar el prompt para tareas especÃ­ficas
- **Costo**: El context window se llena con definiciones de tools no relevantes

La soluciÃ³n: **mÃºltiples agentes especializados** orquestados por un **Router Agent**.

### PatrÃ³n Router â†’ Specialists

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  User query â”€â”€â”€â”€â”€â”€â–¶â”‚  ROUTER AGENT   â”‚
                    â”‚  (clasificador) â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                   Â¿QuÃ© tipo de tarea?
                            â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚             â”‚             â”‚
        â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
        â”‚    RAG    â”‚ â”‚   DATA   â”‚ â”‚  SUMMARY  â”‚
        â”‚ Specialistâ”‚ â”‚Specialistâ”‚ â”‚ Specialistâ”‚
        â”‚           â”‚ â”‚          â”‚ â”‚           â”‚
        â”‚ - search  â”‚ â”‚ - stats  â”‚ â”‚ - resumir â”‚
        â”‚ - docs    â”‚ â”‚ - analyzeâ”‚ â”‚ - extract â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### El Router Agent

Es un agente **muy liviano** con un prompt restrictivo:

```
"Responde SOLO con el nombre del especialista: RAG, DATA, o SUMMARY."
```

No tiene tools. Solo clasifica la consulta. Esto es eficiente porque:
- Usa pocos tokens (prompt corto + output de 1 palabra)
- Es muy rÃ¡pido (single-shot, sin reasoning loop)
- Se puede usar un modelo mÃ¡s barato para este paso

### Ventajas del patrÃ³n

1. **Prompts optimizados**: Cada especialista tiene un system prompt diseÃ±ado para
   su tarea especÃ­fica
2. **Menos tools por agente**: 2-3 tools vs. 10-20 â†’ el LLM elige mejor
3. **Modelos diferentes por agente**: Router con modelo barato, especialistas con modelo potente
4. **Escalabilidad**: Agregar un nuevo especialista no afecta a los existentes
5. **Debugging mÃ¡s fÃ¡cil**: Si falla, sabÃ©s exactamente quÃ© especialista y por quÃ©

### Estado actual en `03-java-rag-agent`

El Router Agent funciona y clasifica correctamente, pero los 3 especialistas delegan
al mismo `RagAgent`. En producciÃ³n, cada `case` del `switch` tendrÃ­a su propio agente
con prompt y tools diferenciados.

---

## Tool Calling: TypeScript vs Java {#tool-calling-typescript-vs-java}

La misma funcionalidad de tool calling se implementa de forma muy diferente en TS
(manual) vs Java (Spring AI):

### TypeScript (Proyectos 01 y 02) â€” Manual

```typescript
// 1. Definir schema JSON manualmente
const definition: ToolDefinition = {
  name: "get_weather",
  description: "...",
  parameters: { type: "object", properties: {...}, required: [...] }
};

// 2. Validar con Zod
const ArgsSchema = z.object({ city: z.string() });

// 3. Implementar execute
async function execute(args) {
  const parsed = ArgsSchema.parse(args);
  return JSON.stringify(resultado);
}

// 4. Reasoning loop manual (while loop)
while (iterations < max) {
  const response = await llm.chat(messages, tools);
  if (response.hasToolCalls) {
    // ejecutar tools, agregar resultados, continuar
  } else {
    // respuesta final, salir
  }
}
```

**Responsabilidades del desarrollador**: JSON Schema, validaciÃ³n, ejecuciÃ³n, loop, manejo de
errores, historial de mensajes.

### Java / Spring AI (Proyecto 03) â€” Declarativo

```java
// 1. Definir como @Bean + Java Record (schema se auto-genera)
@Bean
@Description("Busca documentos relevantes en la base de conocimiento")
public Function<SearchRequest, SearchResponse> searchDocuments() {
    return request -> {
        String context = retrievalService.searchAndFormat(request.query(), request.maxResults());
        return new SearchResponse(context, !context.contains("No se encontraron"));
    };
}

// 2. Registrar por nombre
chatClient = builder.defaultFunctions("searchDocuments", "analyzeData").build();

// 3. Usar (Spring AI maneja el loop internamente)
String response = chatClient.prompt().user(query).call().content();
```

**Spring AI se encarga de**: JSON Schema (auto-generado desde Java records), reasoning loop,
parseo de tool calls, re-invocaciÃ³n del LLM, historial de mensajes.

### Comparativa

| Aspecto | TypeScript (manual) | Java / Spring AI |
|---------|--------------------|--------------------|
| **JSON Schema** | Manual | Auto-generado desde Records |
| **ValidaciÃ³n** | Zod | Java type system + Bean Validation |
| **Reasoning Loop** | `while` loop explÃ­cito | Interno (abstractido) |
| **Registro de tools** | Array + `findTool()` | `@Bean` + `defaultFunctions()` |
| **Control** | Total â€” ves cada paso | Menos â€” Spring AI decide internamente |
| **Debugging** | FÃ¡cil â€” logs en cada paso | MÃ¡s opaco â€” hay que activar logging de Spring AI |
| **Flexibilidad** | MÃ¡xima | Limitada al API de Spring AI |
| **Boilerplate** | MÃ¡s cÃ³digo | Menos cÃ³digo |
| **Mejor para** | Aprender cÃ³mo funciona | ProducciÃ³n enterprise |

**RecomendaciÃ³n**: EmpezÃ¡ con TypeScript (01-cli-agent) para entender el mecanismo completo.
DespuÃ©s pasÃ¡ a Spring AI (03) donde todo es mÃ¡s fÃ¡cil pero mÃ¡s opaco.

---

## Patrones de error y recuperaciÃ³n {#patrones-de-error-y-recuperacion}

### Principio fundamental: errores devueltos, no lanzados

Cuando un tool falla, el error **no se lanza** como excepciÃ³n. Se **devuelve como string**
al LLM:

```typescript
// âŒ Mal â€” el agente crashea
throw new Error("API no disponible");

// âœ… Bien â€” el LLM puede recuperarse
return JSON.stringify({ error: "API no disponible. Intenta otra ciudad." });
```

Â¿Por quÃ©? Porque el LLM puede **decidir quÃ© hacer** con el error:
- Intentar con otros argumentos
- Usar un tool alternativo
- Informar al usuario del problema

### ImplementaciÃ³n por proyecto

- **`01-cli-agent`** (`agent.ts`): `try/catch` en `executeTool()`, devuelve
  `JSON.stringify({ error })` al LLM
- **`02-generative-ui`** (`agent.ts`): Mismo patrÃ³n + emite un evento SSE de tipo
  `"error"` para que el frontend muestre feedback visual
- **`03-java-rag-agent`**: Spring AI maneja errores internamente

### ProtecciÃ³n anti-loop

AdemÃ¡s de errores de tools, existe el riesgo de un **loop infinito** donde el LLM
llama tools repetidamente sin converger a una respuesta. La soluciÃ³n: `maxIterations`.

Si se alcanza el lÃ­mite, el agente retorna un mensaje de error al usuario sugiriendo
reformular la pregunta. Esto es preferible a un timeout silencioso.

---

## Multi-Provider: configuraciÃ³n compartida {#multi-provider-configuracion-compartida}

Los 3 proyectos comparten el mismo archivo `.env` en la raÃ­z del workspace y soportan
los mismos 5 proveedores de LLM. Esto permite cambiar de proveedor sin tocar cÃ³digo.

### Proveedores soportados

| Provider | Base URL | Chat Model | Embeddings | Gratis |
|----------|----------|-----------|------------|--------|
| **ollama** | `localhost:11434` | llama3.1 | nomic-embed-text | SÃ­ (local) |
| **groq** | `api.groq.com` | llama-3.1-70b | âš ï¸ via Ollama | SÃ­ (cloud) |
| **gemini** | `generativelanguage.googleapis.com` | gemini-2.0-flash | âš ï¸ via Ollama | SÃ­ (cloud) |
| **openai** | `api.openai.com` | gpt-4o-mini | text-embedding-3-small | No |
| **github** | `models.inference.ai.azure.com` | gpt-4o-mini | text-embedding-3-small | SÃ­ (con cuenta) |

### El truco: API compatible con OpenAI

Todos estos proveedores exponen una API compatible con el formato de OpenAI (`/v1/chat/completions`).
Eso permite usar el **mismo SDK** cambiando solo `baseURL` y `apiKey`:

```
// TypeScript (proyectos 01 y 02)
new OpenAI({ apiKey: "...", baseURL: "https://api.groq.com/openai/v1" })

// Java (proyecto 03, via application.yml)
spring.ai.openai.base-url=${OPENAI_BASE_URL}
spring.ai.openai.api-key=${OPENAI_API_KEY}
```

### Embeddings: el caso especial de Groq y Gemini

No todos los proveedores soportan el endpoint `/v1/embeddings`. Para los que no
(Groq y Gemini), el proyecto Java usa un **endpoint de embedding separado**
apuntando a Ollama local:

```
Chat:       Groq  (api.groq.com)       â”€â”€â”€ rÃ¡pido, cloud
Embeddings: Ollama (localhost:11434)   â”€â”€â”€ local, sin costo
```

Esto se configura en `RagConfig.java` con un `EmbeddingModel` bean `@Primary` que
recibe su propia `base-url` y `api-key` independientes del chat.

### Flujo de carga de configuraciÃ³n (proyecto Java)

```
1. main() â†’ EnvLoader.load()
   Lee ../.env, setea System properties

2. main() â†’ ProviderResolver.resolve()
   Lee PROVIDER, mapea a URLs/modelos
   Setea: OPENAI_BASE_URL, MODEL, EMBEDDING_BASE_URL, etc.

3. SpringApplication.run()
   application.yml lee ${OPENAI_BASE_URL}, ${MODEL}, etc.
   RagConfig crea EmbeddingModel con endpoint independiente
```

---

## CÃ³mo testear el proyecto 03 (Java RAG) {#como-testear-proyecto-03}

### Requisitos previos

1. **Java 21** â€” `java --version`
2. **Ollama** (recomendado) â€” `curl -fsSL https://ollama.com/install.sh | sh`
3. Modelos descargados: `ollama pull llama3.1 && ollama pull nomic-embed-text`

### Arrancar el proyecto

```bash
# Desde la raÃ­z del workspace
cd 03-java-rag-agent

# OpciÃ³n A: con Ollama (default, sin .env necesario)
./mvnw spring-boot:run

# OpciÃ³n B: con otro provider
PROVIDER=groq OPENAI_API_KEY=gsk_... ./mvnw spring-boot:run

# OpciÃ³n C: con .env configurado
# (configurar ../.env y luego)
./mvnw spring-boot:run
```

### Frontend visual

Al arrancar, abrir **http://localhost:8080** en el navegador.
Spring Boot sirve automÃ¡ticamente el `index.html` desde `src/main/resources/static/`.

El frontend incluye:
- **Chat** con el agente RAG
- Toggle entre **RAG Chat** y **Multi-Agent**
- **Upload de documentos** para el pipeline RAG
- **Ingestar directorio** completo
- VisualizaciÃ³n de **fuentes/contexto** usado en cada respuesta
- Badge con el **proveedor** y **modelo** activo

### Testear con curl

```bash
# Info del proveedor activo
curl http://localhost:8080/api/info | jq

# Chat simple
curl -X POST http://localhost:8080/api/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "Â¿QuÃ© es RAG?"}'

# Multi-agent orchestration
curl -X POST http://localhost:8080/api/orchestrate \
  -H "Content-Type: application/json" \
  -d '{"message": "Resume los documentos"}'

# Subir un documento
curl -X POST http://localhost:8080/api/documents/upload \
  -F "file=@mi-documento.pdf"

# Ingestar todos los documentos del directorio ./documents
curl -X POST http://localhost:8080/api/documents/ingest-all

# Reset conversaciÃ³n
curl -X POST http://localhost:8080/api/reset
```

### Flujo de test recomendado

1. **Arrancar** el servidor
2. **Crear** un archivo de prueba: `echo "Spring AI es un framework..." > documents/test.txt`
3. **Ingestar**: click en "Ingestar directorio" (o `curl -X POST .../ingest-all`)
4. **Preguntar**: "Â¿QuÃ© es Spring AI?" â†’ deberÃ­a responder usando el documento
5. **Verificar fuentes**: expandir "Ver fuentes/contexto" en la respuesta
6. **Probar Multi-Agent**: cambiar modo y hacer otra pregunta

---

## Glosario rÃ¡pido {#glosario-rapido}

| TÃ©rmino | DefiniciÃ³n |
|---------|-----------|
| **LLM** | Large Language Model â€” modelo de lenguaje (GPT-4, Llama, Gemini) |
| **Tool** | FunciÃ³n que el LLM puede invocar (no ejecuta â€” solo genera el JSON) |
| **Tool Call** | JSON que el LLM genera declarando quÃ© funciÃ³n quiere ejecutar |
| **Reasoning Loop** | Ciclo thinkâ†’actâ†’observe que el agente repite hasta responder |
| **RAG** | Retrieval-Augmented Generation â€” inyectar documentos en el prompt |
| **Embedding** | Vector numÃ©rico que representa el significado de un texto |
| **Vector Store** | Base de datos de embeddings con bÃºsqueda por similitud |
| **Chunk** | Fragmento de un documento (tÃ­p. 500-1000 tokens) |
| **Similitud coseno** | MÃ©trica de distancia entre vectores (1.0 = idÃ©nticos) |
| **SSE** | Server-Sent Events â€” protocolo de eventos servidorâ†’cliente |
| **Generative UI** | PatrÃ³n donde el agente decide quÃ© componentes UI renderizar |
| **UIAction** | InstrucciÃ³n del agente al frontend: mount/update/unmount |
| **Router Agent** | Agente liviano que clasifica consultas y delega a especialistas |
| **System Prompt** | Instrucciones iniciales que definen el comportamiento del agente |
| **tool_choice** | ParÃ¡metro que controla si el LLM puede/debe usar tools |
| **Zod** | LibrerÃ­a de validaciÃ³n runtime para TypeScript |
| **Zustand** | State manager minimalista para React |
| **Spring AI** | Framework de Spring para integraciÃ³n con LLMs |
| **Apache Tika** | LibrerÃ­a de parsing de documentos (PDF, DOCX, etc.) |
| **Chain-of-thought** | TÃ©cnica de prompting: "piensa paso a paso" |
| **Few-shot** | TÃ©cnica de prompting: dar ejemplos en el prompt |
| **Context window** | MÃ¡ximo de tokens que un LLM puede procesar en un request |
| **Temperature** | ParÃ¡metro que controla la creatividad del LLM (0=determinista, 1=creativo) |
